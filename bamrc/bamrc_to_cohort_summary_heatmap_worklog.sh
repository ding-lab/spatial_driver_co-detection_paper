
# compute1
# bam-rc genotyping
cd /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/
bsub -G compute-dinglab -q dinglab -oo bamrc_job_submission.log -a 'docker(scao/dailybox)' \
bash /storage1/fs1/dinglab/Active/Projects/austins2/tools/bamrc/bam-rc_wrapper-cp1.sh \
-R /storage1/fs1/songcao/Active/Database/hg38_database/GRCh38.d1.vd1/GRCh38.d1.vd1.fa \
-l /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/snv_probe_sites.tsv \
-I /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/bamrc_input_table_2024-06-05.tsv \
-O /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/
# wait for above jobs to finish then run the following which generates the sample_ID.readcounts.parsed.tsv file.
for file in $(ls /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/*/*.readcounts.tsv); do cat $file | awk -F "[\t:]" '{OFS="\t"; print $1, $2, $3, $4, $19, $20, $33, $34, $47, $48, $61, $62, $89, $90}' > ${file/tsv/parsed.tsv}; done
# compute1
cd /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-07-11/
bsub -G compute-dinglab -q dinglab -oo bamrc_job_submission.log -a 'docker(scao/dailybox)' \
bash /storage1/fs1/dinglab/Active/Projects/austins2/tools/bamrc/bam-rc_wrapper-cp1.sh \
-R /storage1/fs1/songcao/Active/Database/hg38_database/GRCh38.d1.vd1/GRCh38.d1.vd1.fa \
-l /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/snv_probe_sites.tsv \
-I /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-07-11/bamrc_input_table_2024-07-11.tsv \
-O /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-07-11/
# wait for above jobs to finish then run the following which generates the sample_ID.readcounts.parsed.tsv file.
for file in $(ls /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-07-11/*/*.readcounts.tsv); do cat $file | awk -F "[\t:]" '{OFS="\t"; print $1, $2, $3, $4, $19, $20, $33, $34, $47, $48, $61, $62, $89, $90}' > ${file/tsv/parsed.tsv}; done
# compute1
cd /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-11-25/
bsub -G compute-dinglab -q dinglab -oo bamrc_job_submission.log -a 'docker(scao/dailybox)' \
bash /storage1/fs1/dinglab/Active/Projects/austins2/tools/bamrc/bam-rc_wrapper-cp1.sh \
-R /storage1/fs1/songcao/Active/Database/hg38_database/GRCh38.d1.vd1/GRCh38.d1.vd1.fa \
-l /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-06-05/snv_probe_sites.tsv \
-I /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-11-25/bamrc_input_table_2024-11-25.tsv \
-O /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-11-25/
# wait for jobs created by the above command to finish then run the following which generates the sample_ID.readcounts.parsed.tsv file.
for file in $(ls /storage1/fs1/dinglab/Active/Projects/austins2/snv_project/bamrc/2024-11-25/*/*.readcounts.tsv); do cat $file | awk -F "[\t:]" '{OFS="\t"; print $1, $2, $3, $4, $19, $20, $33, $34, $47, $48, $61, $62, $89, $90}' > ${file/tsv/parsed.tsv}; done
# checked each parsed mutation file manually to determine which mutations are in the seqeuncing data following the procedure outlined here: https://pmc.ncbi.nlm.nih.gov/articles/PMC6450397/ 
# transfered all bamrc output files back over to lab cluster from wustl RIS compute1 at the following path:
# /diskmnt/Projects/HTAN_analysis_2/PDAC/xenium/snvs_project/bamrc_output/
# The py3.9 environment is the same environment as used for running automated scrublet doublet detection https://github.com/Aust1nS2/automated_scrublet
# generated the bamrc_vaf_summary_table.tsv via the following commands:
# lab_server
conda activate py3.9
cd /diskmnt/Projects/HTAN_analysis_2/PDAC/xenium/snvs_project/bamrc_output/vaf_summary_table/v3_freeze/
python3 bamrc_to_vaf_table_v3.py \
/diskmnt/Projects/HTAN_analysis_2/PDAC/xenium/snvs_project/bamrc_output/vaf_summary_table/v3_freeze/variant_look_up_table.tsv \
/diskmnt/Projects/HTAN_analysis_2/PDAC/xenium/snvs_project/bamrc_output/vaf_summary_table/v3_freeze/input_bamrc_counts_files.tsv \
/diskmnt/Projects/HTAN_analysis_2/PDAC/xenium/snvs_project/bamrc_output/vaf_summary_table/v3_freeze/
# the output file "complete_parsed_bamrc_result_file.tsv" generated by the python script is supplementary table 2
# the output file "bamrc_vaf_summary_table.tsv" generated by the python script is the direct input used to make figure 1 cohort summary heatmap
# lab_server
conda activate seurat5
Rscript plotting_summary_heatmap_v2.R